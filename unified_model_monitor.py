#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
í†µí•© ëª¨ë¸ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ
- ëª¨ë“  ì‹¤í—˜ì— ëŒ€í•œ ë°ì´í„° ë¶„ì„ ë° ì‹œê°í™”ë¥¼ í•˜ë‚˜ì˜ ìŠ¤í¬ë¦½íŠ¸ë¡œ ì²˜ë¦¬
- CSV íŒŒì¼ ìƒì„± ë° ëª¨ë“  ê·¸ë˜í”„ ìë™ ìƒì„±
- /model_result_monitoring/ì‹¤í—˜_ëª¨ë¸_id ê²½ë¡œì— ê²°ê³¼ ì €ì¥
"""

import os
import sys
import pandas as pd #type: ignore
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns #type: ignore
from pathlib import Path
import argparse
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

class ModelMonitor:
    """í†µí•© ëª¨ë¸ ëª¨ë‹ˆí„°ë§ í´ë˜ìŠ¤"""
    
    def __init__(self, workspace_dir, output_base_dir="model_result_monitoring"):
        self.workspace_dir = Path(workspace_dir)
        self.graph_dir = self.workspace_dir / "graph"
        self.output_base_dir = self.workspace_dir / output_base_dir
        self.output_base_dir.mkdir(exist_ok=True)
        
        # ê·¸ë˜í”„ ìŠ¤íƒ€ì¼ ì„¤ì •
        plt.style.use('seaborn-v0_8')
        sns.set_palette("husl")
        
    def discover_experiments(self):
        """ëª¨ë“  ì‹¤í—˜ ë°ì´í„°ë¥¼ ë°œê²¬í•˜ê³  ì •ë¦¬"""
        experiments = []
        
        if not self.graph_dir.exists():
            print(f"âŒ ê·¸ë˜í”„ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {self.graph_dir}")
            return experiments
        
        # ìƒˆ í˜•ì‹: graph/<ì‹¤í—˜ID>/ í˜•íƒœ íƒìƒ‰
        for exp_dir in self.graph_dir.iterdir():
            if exp_dir.is_dir():
                epoch_file = exp_dir / "epoch-loss.txt"
                batch_file = exp_dir / "batch-loss.txt"
                
                if epoch_file.exists() and batch_file.exists():
                    experiments.append({
                        'id': exp_dir.name,
                        'path': exp_dir,
                        'epoch_file': epoch_file,
                        'batch_file': batch_file,
                        'format': 'directory'
                    })
        
        # êµ¬ í˜•ì‹: graph/epoch-loss-<ID>.txt í˜•íƒœ íƒìƒ‰ (í´ë°±)
        if not experiments:
            epoch_files = list(self.graph_dir.glob("epoch-loss-*.txt"))
            for epoch_file in epoch_files:
                exp_id = epoch_file.stem.replace("epoch-loss-", "")
                batch_file = self.graph_dir / f"batch-loss-{exp_id}.txt"
                
                if batch_file.exists():
                    experiments.append({
                        'id': exp_id,
                        'path': self.graph_dir,
                        'epoch_file': epoch_file,
                        'batch_file': batch_file,
                        'format': 'flat'
                    })
        
        # ìˆ˜ì • ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ìµœì‹  ìˆœ)
        experiments.sort(key=lambda x: x['epoch_file'].stat().st_mtime, reverse=True)
        
        print(f"ğŸ” ë°œê²¬ëœ ì‹¤í—˜: {len(experiments)}ê°œ")
        for exp in experiments:
            print(f"   - {exp['id']} ({exp['format']} format)")
        
        return experiments
    
    def load_experiment_data(self, experiment):
        """ì‹¤í—˜ ë°ì´í„° ë¡œë“œ"""
        try:
            # New format with header and comma separator
            if experiment['format'] == 'directory':
                epoch_df = pd.read_csv(experiment['epoch_file']) # sep=',' is default
                batch_df = pd.read_csv(experiment['batch_file'])
                # Ensure timestamp columns are parsed as strings (not datetime for display purposes)
                if 'timestamp' in epoch_df.columns:
                    epoch_df['timestamp'] = epoch_df['timestamp'].astype(str)
                if 'timestamp' in batch_df.columns:
                    batch_df['timestamp'] = batch_df['timestamp'].astype(str)
                # Convert empty val_loss to NaN for visualization
                if 'val_loss' in epoch_df.columns:
                    epoch_df['val_loss'] = pd.to_numeric(epoch_df['val_loss'], errors='coerce')
            # Old format with space separator and no header
            else: # format == 'flat'
                epoch_df = pd.read_csv(experiment['epoch_file'], sep=' ', header=None,
                                     names=['epoch', 'avg_loss'])
                batch_df = pd.read_csv(experiment['batch_file'], sep=' ', header=None,
                                     names=['epoch', 'batch_num', 'loss'])

            return epoch_df, batch_df
        
        except Exception as e:
            print(f"âŒ {experiment['id']} ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None, None
    
    def calculate_epoch_statistics(self, batch_df):
        """ì—í­ë³„ ë°°ì¹˜ ì†ì‹¤ í†µê³„ ê³„ì‚°"""
        epoch_stats = batch_df.groupby('epoch')['loss'].agg([
            'count',      # ë°°ì¹˜ ìˆ˜
            'mean',       # í‰ê· 
            'std',        # í‘œì¤€í¸ì°¨
            'var',        # ë¶„ì‚°
            'min',        # ìµœì†Ÿê°’
            'max',        # ìµœëŒ“ê°’
            'median'      # ì¤‘ì•™ê°’
        ]).reset_index()
        
        # ì¶”ê°€ í†µê³„ (0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€)
        epoch_stats['cv'] = epoch_stats.apply(
            lambda row: row['std'] / row['mean'] if row['mean'] > 0 and row['std'] > 0 else 0, axis=1
        )
        epoch_stats['range'] = epoch_stats['max'] - epoch_stats['min']  # ë²”ìœ„
        
        # NaNì„ 0ìœ¼ë¡œ ëŒ€ì²´
        epoch_stats.fillna(0, inplace=True)
        
        return epoch_stats
    
    def calculate_gradient_analysis(self, epoch_df):
        """ê¸°ìš¸ê¸° ì†Œì‹¤ ë¶„ì„"""
        # Handle both old and new column names
        loss_col = 'avg_loss' if 'avg_loss' in epoch_df.columns else 'loss'
        epoch_col = 'epoch'
        
        epochs = epoch_df[epoch_col].values
        losses = epoch_df[loss_col].values
        
        # 1ì°¨, 2ì°¨ ë¯¸ë¶„ ê³„ì‚°
        dloss_depoch = np.gradient(losses, epochs)
        d2loss_depoch2 = np.gradient(dloss_depoch, epochs)
        
        # ê¸°ìš¸ê¸° ì†Œì‹¤ ë¶„ì„
        abs_gradient = np.abs(dloss_depoch)
        final_gradient_magnitude = np.mean(abs_gradient[-50:]) if len(abs_gradient) >= 50 else np.mean(abs_gradient[-10:])
        initial_gradient_magnitude = np.mean(abs_gradient[:50]) if len(abs_gradient) >= 50 else np.mean(abs_gradient[:10])
        
        gradient_ratio = final_gradient_magnitude / initial_gradient_magnitude if initial_gradient_magnitude > 0 else 0
        
        # ìˆ˜ë ´ ìƒíƒœ íŒì •
        if gradient_ratio < 0.01:
            convergence_status = "ì‹¬ê°í•œ ê¸°ìš¸ê¸° ì†Œì‹¤"
        elif gradient_ratio < 0.1:
            convergence_status = "ê¸°ìš¸ê¸° ì†Œì‹¤ ê°€ëŠ¥ì„±"
        elif gradient_ratio < 0.5:
            convergence_status = "ì •ìƒì ì¸ ìˆ˜ë ´"
        else:
            convergence_status = "í™œë°œí•œ í•™ìŠµ"
        
        result = {
            'dloss_depoch': dloss_depoch,
            'd2loss_depoch2': d2loss_depoch2,
            'abs_gradient': abs_gradient,
            'gradient_ratio': gradient_ratio,
            'final_gradient_magnitude': final_gradient_magnitude,
            'initial_gradient_magnitude': initial_gradient_magnitude,
            'convergence_status': convergence_status,
            'recent_variance': np.var(losses[-100:]) if len(losses) >= 100 else np.var(losses),
            'total_improvement': losses[0] - losses[-1] if len(losses) > 0 else 0,
            'loss_col': loss_col
        }
        
        # Validation loss ë¶„ì„ (ìˆëŠ” ê²½ìš°)
        if 'val_loss' in epoch_df.columns:
            valid_indices = epoch_df['val_loss'].notna()
            if valid_indices.any():
                val_epochs = epoch_df.loc[valid_indices, epoch_col].values
                val_losses = epoch_df.loc[valid_indices, 'val_loss'].values
                
                if len(val_losses) > 1:
                    val_dloss_depoch = np.gradient(val_losses, val_epochs)
                    val_d2loss_depoch2 = np.gradient(val_dloss_depoch, val_epochs)
                    val_abs_gradient = np.abs(val_dloss_depoch)
                    
                    val_final_grad = np.mean(val_abs_gradient[-50:]) if len(val_abs_gradient) >= 50 else np.mean(val_abs_gradient[-10:])
                    val_initial_grad = np.mean(val_abs_gradient[:50]) if len(val_abs_gradient) >= 50 else np.mean(val_abs_gradient[:10])
                    val_gradient_ratio = val_final_grad / val_initial_grad if val_initial_grad > 0 else 0
                    
                    result.update({
                        'val_dloss_depoch': val_dloss_depoch,
                        'val_d2loss_depoch2': val_d2loss_depoch2,
                        'val_abs_gradient': val_abs_gradient,
                        'val_gradient_ratio': val_gradient_ratio,
                        'val_epochs': val_epochs,
                        'val_losses': val_losses,
                        'val_total_improvement': val_losses[0] - val_losses[-1]
                    })
        
        return result
    
    def generate_comprehensive_csv(self, experiment_id, epoch_df, batch_df, epoch_stats, gradient_analysis, output_dir):
        """ì¢…í•© CSV íŒŒì¼ ìƒì„±"""
        csv_files = {}
        
        # Determine loss column name
        loss_col = gradient_analysis.get('loss_col', 'avg_loss')
        
        # CSV ì „ìš© ë””ë ‰í† ë¦¬ ìƒì„±
        csv_dir = output_dir / "csv_data"
        csv_dir.mkdir(exist_ok=True)
        
        # 1. ê¸°ë³¸ ì—í­ ë°ì´í„°
        epoch_enhanced = epoch_df.copy()
        epoch_enhanced['gradient'] = gradient_analysis['dloss_depoch']
        epoch_enhanced['gradient_2nd'] = gradient_analysis['d2loss_depoch2']
        epoch_enhanced['abs_gradient'] = gradient_analysis['abs_gradient']
        
        csv_files['epoch_data'] = csv_dir / f"epoch_comprehensive_{experiment_id}.csv"
        epoch_enhanced.to_csv(csv_files['epoch_data'], index=False, encoding='utf-8')
        
        # 2. ì—í­ í†µê³„
        csv_files['epoch_statistics'] = csv_dir / f"epoch_statistics_{experiment_id}.csv"
        epoch_stats.to_csv(csv_files['epoch_statistics'], index=False, encoding='utf-8')
        
        # 3. ë°°ì¹˜ ë°ì´í„° (ìƒ˜í”Œë§)
        if len(batch_df) > 10000:  # ë„ˆë¬´ í¬ë©´ ìƒ˜í”Œë§
            batch_sample = batch_df.sample(n=10000, random_state=42).sort_values(['epoch', 'batch_num'])
        else:
            batch_sample = batch_df
        
        csv_files['batch_data'] = csv_dir / f"batch_data_{experiment_id}.csv"
        batch_sample.to_csv(csv_files['batch_data'], index=False, encoding='utf-8')
        
        # 4. ìš”ì•½ í†µê³„
        summary_data = {
            'experiment_id': [experiment_id],
            'total_epochs': [len(epoch_df)],
            'total_batches': [len(batch_df)],
            'initial_loss': [epoch_df[loss_col].iloc[0]],
            'final_loss': [epoch_df[loss_col].iloc[-1]],
            'min_loss': [epoch_df[loss_col].min()],
            'gradient_ratio': [gradient_analysis['gradient_ratio']],
            'convergence_status': [gradient_analysis['convergence_status']],
            'total_improvement': [gradient_analysis['total_improvement']],
            'avg_batch_std': [epoch_stats['std'].mean()],
            'avg_batch_var': [epoch_stats['var'].mean()],
            'generation_time': [datetime.now().strftime('%Y-%m-%d %H:%M:%S')]
        }
        
        summary_df = pd.DataFrame(summary_data)
        csv_files['summary'] = csv_dir / f"experiment_summary_{experiment_id}.csv"
        summary_df.to_csv(csv_files['summary'], index=False, encoding='utf-8')
        
        return csv_files
    
    def create_loss_plots(self, experiment_id, epoch_df, batch_df, output_dir):
        """ì†ì‹¤ ê´€ë ¨ ê·¸ë˜í”„ ìƒì„±"""
        # ê·¸ë˜í”„ ì „ìš© ë””ë ‰í† ë¦¬ ìƒì„±
        plots_dir = output_dir / "plots"
        plots_dir.mkdir(exist_ok=True)
        
        # Determine loss column name
        loss_col = 'avg_loss' if 'avg_loss' in epoch_df.columns else 'loss'
        
        # 1. ì—í­ í‰ê·  ì†ì‹¤ (with optional validation loss)
        fig, ax = plt.subplots(figsize=(12, 8))
        ax.plot(epoch_df['epoch'], epoch_df[loss_col], 'b-', linewidth=2, marker='o', markersize=3, label='Training Loss')
        
        # Add validation loss if available
        if 'val_loss' in epoch_df.columns:
            # Filter out NaN values for validation loss
            valid_indices = epoch_df['val_loss'].notna()
            if valid_indices.any():
                ax.plot(epoch_df.loc[valid_indices, 'epoch'], 
                       epoch_df.loc[valid_indices, 'val_loss'], 
                       'r--', linewidth=2, marker='s', markersize=3, label='Validation Loss')
                ax.legend(fontsize=12)
        
        ax.set_title(f'Epoch Loss - {experiment_id}', fontsize=16, fontweight='bold')
        ax.set_xlabel('Epoch', fontsize=14)
        ax.set_ylabel('Loss', fontsize=14)
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        loss_plot_path = plots_dir / f"epoch_loss_{experiment_id}.png"
        plt.savefig(loss_plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        # 1-2. Validation Loss Only ê·¸ë˜í”„ (val_lossê°€ ìˆëŠ” ê²½ìš°)
        val_loss_plot_path = None
        if 'val_loss' in epoch_df.columns:
            valid_indices = epoch_df['val_loss'].notna()
            if valid_indices.any():
                fig, ax = plt.subplots(figsize=(12, 8))
                ax.plot(epoch_df.loc[valid_indices, 'epoch'], 
                       epoch_df.loc[valid_indices, 'val_loss'], 
                       'r-', linewidth=2, marker='s', markersize=4, label='Validation Loss')
                ax.set_title(f'Epoch Validation Loss - {experiment_id}', fontsize=16, fontweight='bold')
                ax.set_xlabel('Epoch', fontsize=14)
                ax.set_ylabel('Validation Loss', fontsize=14)
                ax.grid(True, alpha=0.3)
                ax.legend(fontsize=12)
                
                plt.tight_layout()
                val_loss_plot_path = plots_dir / f"epoch_val_loss_{experiment_id}.png"
                plt.savefig(val_loss_plot_path, dpi=300, bbox_inches='tight')
                plt.close()
        
        # 2. ì„ íƒëœ ì—í­ì˜ ë°°ì¹˜ ì†ì‹¤
        total_epochs = epoch_df['epoch'].max()
        
        # ë™ì  ì—í­ ì„ íƒ
        if total_epochs <= 6:
            selected_epochs = list(range(1, total_epochs + 1))
        else:
            selected_epochs = [
                1,
                max(1, total_epochs // 10),
                max(1, total_epochs // 5),
                max(1, total_epochs * 3 // 10),
                max(1, total_epochs // 2),
                max(1, total_epochs * 8 // 10),
                total_epochs
            ]
            selected_epochs = sorted(list(set(selected_epochs)))
        
        # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì—í­ë§Œ í•„í„°ë§
        available_epochs = batch_df['epoch'].unique()
        selected_epochs = [e for e in selected_epochs if e in available_epochs]
        
        if selected_epochs:
            fig, ax = plt.subplots(figsize=(15, 10))
            
            colors = plt.cm.tab10(np.linspace(0, 1, len(selected_epochs))) #type: ignore
            
            for i, epoch in enumerate(selected_epochs):
                epoch_batches = batch_df[batch_df['epoch'] == epoch]
                ax.plot(epoch_batches['batch_num'], epoch_batches['loss'], 
                       color=colors[i], linewidth=1.5, marker='o', markersize=2,
                       label=f'Epoch {epoch}', alpha=0.8)
            
            ax.set_title(f'Batch Loss Comparison - {experiment_id}', fontsize=16, fontweight='bold')
            ax.set_xlabel('Batch Number', fontsize=14)
            ax.set_ylabel('Loss', fontsize=14)
            ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
            ax.grid(True, alpha=0.3)
            
            plt.tight_layout()
            batch_plot_path = plots_dir / f"batch_loss_{experiment_id}.png"
            plt.savefig(batch_plot_path, dpi=300, bbox_inches='tight')
            plt.close()
        
        # 2-2. Validation Lossê°€ ìˆëŠ” ì—í­ë“¤ì˜ ë°°ì¹˜ ì†ì‹¤ ê·¸ë˜í”„
        batch_val_plot_path = None
        if 'val_loss' in epoch_df.columns:
            valid_val_indices = epoch_df['val_loss'].notna()
            if valid_val_indices.any():
                val_epochs = epoch_df.loc[valid_val_indices, 'epoch'].values
                val_selected_epochs = [e for e in val_epochs if e in available_epochs]
                
                # ë™ì  ì—í­ ì„ íƒ (validation epochs ì¤‘ì—ì„œ)
                total_val_epochs = len(val_selected_epochs)
                if total_val_epochs <= 6:
                    filtered_val_epochs = val_selected_epochs
                else:
                    # ê· ë“±í•˜ê²Œ ì„ íƒ
                    indices = np.linspace(0, total_val_epochs - 1, min(7, total_val_epochs), dtype=int)
                    filtered_val_epochs = [val_selected_epochs[i] for i in indices]
                
                if filtered_val_epochs:
                    fig, ax = plt.subplots(figsize=(15, 10))
                    colors_val = plt.cm.Reds(np.linspace(0.4, 0.9, len(filtered_val_epochs))) #type: ignore
                    
                    for i, epoch in enumerate(filtered_val_epochs):
                        epoch_batches = batch_df[batch_df['epoch'] == epoch]
                        ax.plot(epoch_batches['batch_num'], epoch_batches['loss'], 
                               color=colors_val[i], linewidth=1.5, marker='s', markersize=2,
                               label=f'Epoch {epoch} (Val)', alpha=0.8)
                    
                    ax.set_title(f'Batch Loss Comparison (Validation Epochs) - {experiment_id}', fontsize=16, fontweight='bold')
                    ax.set_xlabel('Batch Number', fontsize=14)
                    ax.set_ylabel('Loss', fontsize=14)
                    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
                    ax.grid(True, alpha=0.3)
                    
                    plt.tight_layout()
                    batch_val_plot_path = plots_dir / f"batch_loss_val_{experiment_id}.png"
                    plt.savefig(batch_val_plot_path, dpi=300, bbox_inches='tight')
                    plt.close()
        
        return loss_plot_path, batch_plot_path if selected_epochs else None
    
    def create_gradient_plots(self, experiment_id, epoch_df, gradient_analysis, output_dir):
        """ê¸°ìš¸ê¸° ë¶„ì„ ê·¸ë˜í”„ ìƒì„±"""
        # ê·¸ë˜í”„ ì „ìš© ë””ë ‰í† ë¦¬ ì‚¬ìš©
        plots_dir = output_dir / "plots"
        plots_dir.mkdir(exist_ok=True)
        
        # Determine loss column name
        loss_col = gradient_analysis.get('loss_col', 'avg_loss')
        
        epochs = epoch_df['epoch'].values
        losses = epoch_df[loss_col].values
        dloss_depoch = gradient_analysis['dloss_depoch']
        d2loss_depoch2 = gradient_analysis['d2loss_depoch2']
        abs_gradient = gradient_analysis['abs_gradient']
        
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        
        # 1. ì›ë³¸ Loss ê³¡ì„ 
        axes[0, 0].plot(epochs, losses, 'b-', linewidth=2)
        axes[0, 0].set_title('Loss vs Epoch')
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('Loss')
        axes[0, 0].grid(True, alpha=0.3)
        
        # 2. 1ì°¨ ë¯¸ë¶„ (ê¸°ìš¸ê¸°)
        axes[0, 1].plot(epochs, dloss_depoch, 'r-', linewidth=2)
        axes[0, 1].set_title('Loss Gradient (dLoss/dEpoch)')
        axes[0, 1].set_xlabel('Epoch')
        axes[0, 1].set_ylabel('Gradient')
        axes[0, 1].grid(True, alpha=0.3)
        axes[0, 1].axhline(y=0, color='k', linestyle='--', alpha=0.5)
        
        # 3. ê¸°ìš¸ê¸° í¬ê¸° (ì ˆëŒ“ê°’, ë¡œê·¸ ìŠ¤ì¼€ì¼)
        axes[1, 0].plot(epochs, abs_gradient, 'g-', linewidth=2)
        axes[1, 0].set_title('Absolute Gradient Magnitude')
        axes[1, 0].set_xlabel('Epoch')
        axes[1, 0].set_ylabel('|Gradient|')
        axes[1, 0].set_yscale('log')
        axes[1, 0].grid(True, alpha=0.3)
        
        # 4. 2ì°¨ ë¯¸ë¶„
        axes[1, 1].plot(epochs, d2loss_depoch2, 'm-', linewidth=2)
        axes[1, 1].set_title('Second Derivative (dÂ²Loss/dEpochÂ²)')
        axes[1, 1].set_xlabel('Epoch')
        axes[1, 1].set_ylabel('Second Derivative')
        axes[1, 1].grid(True, alpha=0.3)
        axes[1, 1].axhline(y=0, color='k', linestyle='--', alpha=0.5)
        
        plt.suptitle(f'Gradient Analysis: {experiment_id}', fontsize=16, fontweight='bold')
        plt.tight_layout()
        
        gradient_plot_path = plots_dir / f"gradient_analysis_{experiment_id}.png"
        plt.savefig(gradient_plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        # Validation loss gradient plot (if available)
        val_gradient_plot_path = None
        if 'val_dloss_depoch' in gradient_analysis:
            val_epochs = gradient_analysis['val_epochs']
            val_losses = gradient_analysis['val_losses']
            val_dloss_depoch = gradient_analysis['val_dloss_depoch']
            val_d2loss_depoch2 = gradient_analysis['val_d2loss_depoch2']
            val_abs_gradient = gradient_analysis['val_abs_gradient']
            
            fig, axes = plt.subplots(2, 2, figsize=(16, 12))
            
            # 1. ì›ë³¸ Validation Loss ê³¡ì„ 
            axes[0, 0].plot(val_epochs, val_losses, 'r-', linewidth=2)
            axes[0, 0].set_title('Validation Loss vs Epoch')
            axes[0, 0].set_xlabel('Epoch')
            axes[0, 0].set_ylabel('Validation Loss')
            axes[0, 0].grid(True, alpha=0.3)
            
            # 2. 1ì°¨ ë¯¸ë¶„ (ê¸°ìš¸ê¸°)
            axes[0, 1].plot(val_epochs, val_dloss_depoch, 'orange', linewidth=2)
            axes[0, 1].set_title('Validation Loss Gradient (dLoss/dEpoch)')
            axes[0, 1].set_xlabel('Epoch')
            axes[0, 1].set_ylabel('Gradient')
            axes[0, 1].grid(True, alpha=0.3)
            axes[0, 1].axhline(y=0, color='k', linestyle='--', alpha=0.5)
            
            # 3. ê¸°ìš¸ê¸° í¬ê¸° (ì ˆëŒ“ê°’, ë¡œê·¸ ìŠ¤ì¼€ì¼)
            axes[1, 0].plot(val_epochs, val_abs_gradient, 'purple', linewidth=2)
            axes[1, 0].set_title('Absolute Validation Gradient Magnitude')
            axes[1, 0].set_xlabel('Epoch')
            axes[1, 0].set_ylabel('|Gradient|')
            axes[1, 0].set_yscale('log')
            axes[1, 0].grid(True, alpha=0.3)
            
            # 4. 2ì°¨ ë¯¸ë¶„
            axes[1, 1].plot(val_epochs, val_d2loss_depoch2, 'brown', linewidth=2)
            axes[1, 1].set_title('Validation Second Derivative (dÂ²Loss/dEpochÂ²)')
            axes[1, 1].set_xlabel('Epoch')
            axes[1, 1].set_ylabel('Second Derivative')
            axes[1, 1].grid(True, alpha=0.3)
            axes[1, 1].axhline(y=0, color='k', linestyle='--', alpha=0.5)
            
            plt.suptitle(f'Validation Gradient Analysis: {experiment_id}', fontsize=16, fontweight='bold')
            plt.tight_layout()
            
            val_gradient_plot_path = plots_dir / f"gradient_analysis_val_{experiment_id}.png"
            plt.savefig(val_gradient_plot_path, dpi=300, bbox_inches='tight')
            plt.close()
        
        return gradient_plot_path
    
    def create_statistics_plots(self, experiment_id, epoch_stats, batch_df, output_dir, epoch_df=None):
        """í†µê³„ ë¶„ì„ ê·¸ë˜í”„ ìƒì„±"""
        # ê·¸ë˜í”„ ì „ìš© ë””ë ‰í† ë¦¬ ì‚¬ìš©
        plots_dir = output_dir / "plots"
        plots_dir.mkdir(exist_ok=True)
        
        # 1. ì—í­ë³„ í†µê³„ íŠ¸ë Œë“œ
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        
        # í‘œì¤€í¸ì°¨ íŠ¸ë Œë“œ
        axes[0, 0].plot(epoch_stats['epoch'], epoch_stats['std'], 'b-', linewidth=2)
        axes[0, 0].set_title('Standard Deviation per Epoch')
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('Standard Deviation')
        axes[0, 0].grid(True, alpha=0.3)
        
        # ë¶„ì‚° íŠ¸ë Œë“œ
        axes[0, 1].plot(epoch_stats['epoch'], epoch_stats['var'], 'r-', linewidth=2)
        axes[0, 1].set_title('Variance per Epoch')
        axes[0, 1].set_xlabel('Epoch')
        axes[0, 1].set_ylabel('Variance')
        axes[0, 1].grid(True, alpha=0.3)
        
        # ë³€ë™ê³„ìˆ˜ íŠ¸ë Œë“œ
        axes[1, 0].plot(epoch_stats['epoch'], epoch_stats['cv'], 'g-', linewidth=2)
        axes[1, 0].set_title('Coefficient of Variation per Epoch')
        axes[1, 0].set_xlabel('Epoch')
        axes[1, 0].set_ylabel('CV (std/mean)')
        axes[1, 0].grid(True, alpha=0.3)
        
        # ë²”ìœ„ íŠ¸ë Œë“œ
        axes[1, 1].plot(epoch_stats['epoch'], epoch_stats['range'], 'm-', linewidth=2)
        axes[1, 1].set_title('Loss Range per Epoch')
        axes[1, 1].set_xlabel('Epoch')
        axes[1, 1].set_ylabel('Range (max - min)')
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.suptitle(f'Statistics Trends: {experiment_id}', fontsize=16, fontweight='bold')
        plt.tight_layout()
        
        stats_plot_path = plots_dir / f"statistics_trends_{experiment_id}.png"
        plt.savefig(stats_plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        # 2. í†µê³„ ë¶„í¬ íˆìŠ¤í† ê·¸ë¨
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        
        # í‘œì¤€í¸ì°¨ ë¶„í¬
        axes[0, 0].hist(epoch_stats['std'], bins=30, alpha=0.7, color='blue', edgecolor='black')
        axes[0, 0].set_title('Distribution of Standard Deviations')
        axes[0, 0].set_xlabel('Standard Deviation')
        axes[0, 0].set_ylabel('Frequency')
        axes[0, 0].grid(True, alpha=0.3)
        
        # ë¶„ì‚° ë¶„í¬
        axes[0, 1].hist(epoch_stats['var'], bins=30, alpha=0.7, color='red', edgecolor='black')
        axes[0, 1].set_title('Distribution of Variances')
        axes[0, 1].set_xlabel('Variance')
        axes[0, 1].set_ylabel('Frequency')
        axes[0, 1].grid(True, alpha=0.3)
        
        # ë³€ë™ê³„ìˆ˜ ë¶„í¬
        axes[1, 0].hist(epoch_stats['cv'], bins=30, alpha=0.7, color='green', edgecolor='black')
        axes[1, 0].set_title('Distribution of Coefficient of Variation')
        axes[1, 0].set_xlabel('CV (std/mean)')
        axes[1, 0].set_ylabel('Frequency')
        axes[1, 0].grid(True, alpha=0.3)
        
        # ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ
        corr_cols = ['mean', 'std', 'var', 'cv', 'range']
        correlation_matrix = epoch_stats[corr_cols].corr()
        im = axes[1, 1].imshow(correlation_matrix, cmap='RdBu_r', aspect='auto', vmin=-1, vmax=1)
        axes[1, 1].set_xticks(range(len(corr_cols)))
        axes[1, 1].set_yticks(range(len(corr_cols)))
        axes[1, 1].set_xticklabels(corr_cols, rotation=45)
        axes[1, 1].set_yticklabels(corr_cols)
        axes[1, 1].set_title('Correlation Matrix')
        
        # ìƒê´€ê´€ê³„ ê°’ í‘œì‹œ
        for i in range(len(corr_cols)):
            for j in range(len(corr_cols)):
                text = axes[1, 1].text(j, i, f'{correlation_matrix.iloc[i, j]:.2f}',
                                     ha="center", va="center", color="black", fontsize=8)
        
        plt.colorbar(im, ax=axes[1, 1], shrink=0.8)
        plt.suptitle(f'Statistics Distributions: {experiment_id}', fontsize=16, fontweight='bold')
        plt.tight_layout()
        
        dist_plot_path = plots_dir / f"statistics_distributions_{experiment_id}.png"
        plt.savefig(dist_plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        # 3. Validation Loss ë¶„í¬ ë¶„ì„ ê·¸ë˜í”„ (val_lossê°€ ìˆëŠ” ê²½ìš°)
        val_dist_plot_path = None
        if epoch_df is not None and 'val_loss' in epoch_df.columns:
            valid_indices = epoch_df['val_loss'].notna()
            if valid_indices.any():
                val_losses = epoch_df.loc[valid_indices, 'val_loss'].values
                
                fig, axes = plt.subplots(2, 2, figsize=(16, 12))
                
                # Validation Loss ë¶„í¬
                axes[0, 0].hist(val_losses, bins=20, alpha=0.7, color='red', edgecolor='black')
                axes[0, 0].set_title('Distribution of Validation Loss')
                axes[0, 0].set_xlabel('Validation Loss')
                axes[0, 0].set_ylabel('Frequency')
                axes[0, 0].grid(True, alpha=0.3)
                
                # Training Lossì™€ Validation Loss ë¹„êµ
                train_loss_col = 'avg_loss' if 'avg_loss' in epoch_df.columns else 'loss'
                train_losses = epoch_df[train_loss_col].values
                axes[0, 1].hist([train_losses, val_losses], label=['Training', 'Validation'], 
                               color=['blue', 'red'], alpha=0.7, edgecolor='black', bins=15)
                axes[0, 1].set_title('Train vs Validation Loss Distribution')
                axes[0, 1].set_xlabel('Loss')
                axes[0, 1].set_ylabel('Frequency')
                axes[0, 1].legend()
                axes[0, 1].grid(True, alpha=0.3)
                
                # Validation Loss Box Plot
                axes[1, 0].boxplot([train_losses, val_losses], labels=['Training', 'Validation'])
                axes[1, 0].set_title('Box Plot: Train vs Validation Loss')
                axes[1, 0].set_ylabel('Loss')
                axes[1, 0].grid(True, alpha=0.3, axis='y')
                
                # Validation Loss í†µê³„
                val_mean = np.mean(val_losses)
                val_std = np.std(val_losses)
                val_min = np.min(val_losses)
                val_max = np.max(val_losses)
                train_mean = np.mean(train_losses)
                train_std = np.std(train_losses)
                
                stats_text = f"Validation Loss Statistics:\n"
                stats_text += f"Mean: {val_mean:.6f}\n"
                stats_text += f"Std: {val_std:.6f}\n"
                stats_text += f"Min: {val_min:.6f}\n"
                stats_text += f"Max: {val_max:.6f}\n\n"
                stats_text += f"Training Loss Statistics:\n"
                stats_text += f"Mean: {train_mean:.6f}\n"
                stats_text += f"Std: {train_std:.6f}"
                
                axes[1, 1].text(0.1, 0.5, stats_text, fontsize=11, verticalalignment='center',
                               family='monospace', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
                axes[1, 1].axis('off')
                
                plt.suptitle(f'Validation Loss Statistics: {experiment_id}', fontsize=16, fontweight='bold')
                plt.tight_layout()
                
                val_dist_plot_path = plots_dir / f"statistics_distributions_val_{experiment_id}.png"
                plt.savefig(val_dist_plot_path, dpi=300, bbox_inches='tight')
                plt.close()
        
        return stats_plot_path, dist_plot_path
    
    def generate_experiment_report(self, experiment_id, epoch_df, batch_df, epoch_stats, gradient_analysis, csv_files, plot_files, output_dir):
        """ì‹¤í—˜ ë¦¬í¬íŠ¸ ìƒì„±"""
        report_path = output_dir / f"experiment_report_{experiment_id}.md"
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(f"# ì‹¤í—˜ ë¦¬í¬íŠ¸: {experiment_id}\n\n")
            f.write(f"**ìƒì„± ì¼ì‹œ**: {datetime.now().strftime('%Yë…„ %mì›” %dì¼ %Hì‹œ %Më¶„')}\n\n")
            
            # ì‹¤í—˜ ê°œìš”
            f.write("## ğŸ“Š ì‹¤í—˜ ê°œìš”\n\n")
            f.write(f"- **ì´ ì—í­ ìˆ˜**: {len(epoch_df)}\n")
            f.write(f"- **ì´ ë°°ì¹˜ ìˆ˜**: {len(batch_df)}\n")
            f.write(f"- **ì´ˆê¸° ì†ì‹¤**: {epoch_df['avg_loss'].iloc[0]:.6f}\n")
            f.write(f"- **ìµœì¢… ì†ì‹¤**: {epoch_df['avg_loss'].iloc[-1]:.6f}\n")
            f.write(f"- **ìµœì†Œ ì†ì‹¤**: {epoch_df['avg_loss'].min():.6f}\n")
            f.write(f"- **ì´ ê°œì„ ëŸ‰**: {gradient_analysis['total_improvement']:.6f}\n\n")
            
            # ê¸°ìš¸ê¸° ë¶„ì„
            f.write("## ğŸ” ê¸°ìš¸ê¸° ë¶„ì„\n\n")
            f.write(f"- **ê¸°ìš¸ê¸° ë¹„ìœ¨ (ìµœì¢…/ì´ˆê¸°)**: {gradient_analysis['gradient_ratio']:.4f}\n")
            f.write(f"- **ìˆ˜ë ´ ìƒíƒœ**: {gradient_analysis['convergence_status']}\n")
            f.write(f"- **ì´ˆê¸° ê¸°ìš¸ê¸° í¬ê¸°**: {gradient_analysis['initial_gradient_magnitude']:.8f}\n")
            f.write(f"- **ìµœì¢… ê¸°ìš¸ê¸° í¬ê¸°**: {gradient_analysis['final_gradient_magnitude']:.8f}\n")
            f.write(f"- **ìµœê·¼ ë¶„ì‚°**: {gradient_analysis['recent_variance']:.8f}\n\n")
            
            # í†µê³„ ìš”ì•½
            f.write("## ğŸ“ˆ ë°°ì¹˜ ì†ì‹¤ í†µê³„\n\n")
            f.write(f"- **í‰ê·  í‘œì¤€í¸ì°¨**: {epoch_stats['std'].mean():.6f}\n")
            f.write(f"- **í‰ê·  ë¶„ì‚°**: {epoch_stats['var'].mean():.6f}\n")
            f.write(f"- **í‰ê·  ë³€ë™ê³„ìˆ˜**: {epoch_stats['cv'].mean():.6f}\n")
            f.write(f"- **í‘œì¤€í¸ì°¨ ë²”ìœ„**: {epoch_stats['std'].min():.6f} ~ {epoch_stats['std'].max():.6f}\n\n")
            
            # ìƒì„±ëœ íŒŒì¼ë“¤
            f.write("## ğŸ“ ìƒì„±ëœ íŒŒì¼ë“¤\n\n")
            f.write("### CSV ë°ì´í„° (csv_data/ í´ë”)\n")
            for file_type, file_path in csv_files.items():
                f.write(f"- **{file_type}**: `csv_data/{file_path.name}`\n")
            
            f.write("\n### ê·¸ë˜í”„ (plots/ í´ë”)\n")
            for plot_desc, plot_path in plot_files.items():
                if plot_path:
                    f.write(f"- **{plot_desc}**: `plots/{plot_path.name}`\n")
            
            f.write(f"\n---\n")
            f.write(f"*ë¦¬í¬íŠ¸ ìƒì„±: Unified Model Monitor v1.0*\n")
        
        return report_path
    
    def process_experiment(self, experiment):
        """ë‹¨ì¼ ì‹¤í—˜ ì²˜ë¦¬"""
        experiment_id = experiment['id']
        print(f"\n{'='*60}")
        print(f"ğŸ“Š ì‹¤í—˜ ì²˜ë¦¬ ì¤‘: {experiment_id}")
        print(f"{'='*60}")
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        output_dir = self.output_base_dir / experiment_id
        output_dir.mkdir(exist_ok=True)
        
        # ë°ì´í„° ë¡œë“œ
        print("ğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘...")
        epoch_df, batch_df = self.load_experiment_data(experiment)
        if epoch_df is None or batch_df is None:
            return None
        
        print(f"   - ì—í­ ë°ì´í„°: {len(epoch_df)} ê°œ")
        print(f"   - ë°°ì¹˜ ë°ì´í„°: {len(batch_df)} ê°œ")
        
        # í†µê³„ ê³„ì‚°
        print("ğŸ”¢ í†µê³„ ê³„ì‚° ì¤‘...")
        epoch_stats = self.calculate_epoch_statistics(batch_df)
        gradient_analysis = self.calculate_gradient_analysis(epoch_df)
        
        # CSV íŒŒì¼ ìƒì„±
        print("ğŸ’¾ CSV íŒŒì¼ ìƒì„± ì¤‘...")
        csv_files = self.generate_comprehensive_csv(
            experiment_id, epoch_df, batch_df, epoch_stats, gradient_analysis, output_dir
        )
        
        # ê·¸ë˜í”„ ìƒì„±
        print("ğŸ“ˆ ê·¸ë˜í”„ ìƒì„± ì¤‘...")
        
        # ì†ì‹¤ ê·¸ë˜í”„
        loss_plot, batch_plot = self.create_loss_plots(experiment_id, epoch_df, batch_df, output_dir)
        # Check if validation loss graph was created
        plots_dir = output_dir / "plots"
        val_loss_plot = plots_dir / f"epoch_val_loss_{experiment_id}.png"
        batch_val_plot = plots_dir / f"batch_loss_val_{experiment_id}.png"
        if val_loss_plot.exists():
            print(f"   âœ… Training + Validation Loss ê·¸ë˜í”„ ìƒì„±ë¨")
            print(f"   âœ… Validation Loss Only ê·¸ë˜í”„ ìƒì„±ë¨")
        if batch_val_plot.exists():
            print(f"   âœ… Batch Loss (Train) ê·¸ë˜í”„ ìƒì„±ë¨")
            print(f"   âœ… Batch Loss (Validation) ê·¸ë˜í”„ ìƒì„±ë¨")
        
        # ê¸°ìš¸ê¸° ê·¸ë˜í”„  
        gradient_plot = self.create_gradient_plots(experiment_id, epoch_df, gradient_analysis, output_dir)
        # Check if validation gradient plot was created
        val_gradient_plot = plots_dir / f"gradient_analysis_val_{experiment_id}.png"
        if val_gradient_plot.exists():
            print(f"   âœ… Training Gradient ë¶„ì„ ìƒì„±ë¨")
            print(f"   âœ… Validation Gradient ë¶„ì„ ìƒì„±ë¨")
        else:
            print(f"   âœ… Training Gradient ë¶„ì„ ìƒì„±ë¨")
        
        # í†µê³„ ê·¸ë˜í”„ (ë°°ì¹˜ê°€ ì¶©ë¶„í•  ë•Œë§Œ)
        avg_batches_per_epoch = epoch_stats['count'].mean()
        if avg_batches_per_epoch > 2:
            stats_plot, dist_plot = self.create_statistics_plots(experiment_id, epoch_stats, batch_df, output_dir, epoch_df)
            # Check if validation statistics distribution plot was created
            val_dist_plot = plots_dir / f"statistics_distributions_val_{experiment_id}.png"
            if val_dist_plot.exists():
                print(f"   âœ… Statistics Trends (Train) ê·¸ë˜í”„ ìƒì„±ë¨")
                print(f"   âœ… Statistics Distributions (Train) ê·¸ë˜í”„ ìƒì„±ë¨")
                print(f"   âœ… Statistics Distributions (Validation) ê·¸ë˜í”„ ìƒì„±ë¨")
            else:
                print(f"   âœ… Statistics Trends (Train) ê·¸ë˜í”„ ìƒì„±ë¨")
                print(f"   âœ… Statistics Distributions (Train) ê·¸ë˜í”„ ìƒì„±ë¨")
        else:
            print(f"âš ï¸  ë°°ì¹˜ ìˆ˜ ë¶€ì¡± (í‰ê·  {avg_batches_per_epoch:.1f}ê°œ/ì—í­) - í†µê³„ ê·¸ë˜í”„ ìƒëµ")
            stats_plot, dist_plot = None, None
            # ë°°ì¹˜ ì†ì‹¤ ë¹„êµ ê·¸ë˜í”„ë„ ìƒëµ
            batch_plot = None
        
        plot_files = {
            "ì—í­ í‰ê·  ì†ì‹¤": loss_plot,
            "ë°°ì¹˜ ì†ì‹¤ ë¹„êµ": batch_plot,
            "ê¸°ìš¸ê¸° ë¶„ì„": gradient_plot,
        }
        if stats_plot:
            plot_files["í†µê³„ íŠ¸ë Œë“œ"] = stats_plot
        if dist_plot:
            plot_files["í†µê³„ ë¶„í¬"] = dist_plot
        
        # ì‹¤í—˜ ë¦¬í¬íŠ¸ ìƒì„±
        print("ğŸ“ ì‹¤í—˜ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
        report_path = self.generate_experiment_report(
            experiment_id, epoch_df, batch_df, epoch_stats, gradient_analysis, csv_files, plot_files, output_dir
        )
        
        print(f"âœ… ì™„ë£Œ! ê²°ê³¼ ì €ì¥: {output_dir}")
        print(f"ğŸ“‹ ë¦¬í¬íŠ¸: {report_path.name}")
        
        return {
            'experiment_id': experiment_id,
            'output_dir': output_dir,
            'epoch_df': epoch_df,
            'batch_df': batch_df,
            'epoch_stats': epoch_stats,
            'gradient_analysis': gradient_analysis,
            'csv_files': csv_files,
            'plot_files': plot_files,
            'report_path': report_path
        }
    
    def generate_comparison_report(self, processed_experiments):
        """ì „ì²´ ì‹¤í—˜ ë¹„êµ ë¦¬í¬íŠ¸ ìƒì„±"""
        if not processed_experiments:
            return
        
        print(f"\n{'='*80}")
        print("ğŸ† ì „ì²´ ì‹¤í—˜ ë¹„êµ ë¦¬í¬íŠ¸ ìƒì„±")
        print(f"{'='*80}")
        
        # ë¹„êµ ë°ì´í„° ìˆ˜ì§‘
        comparison_data = []
        for exp in processed_experiments:
            comparison_data.append({
                'experiment_id': exp['experiment_id'],
                'total_epochs': len(exp['epoch_df']),
                'initial_loss': exp['epoch_df']['avg_loss'].iloc[0],
                'final_loss': exp['epoch_df']['avg_loss'].iloc[-1],
                'min_loss': exp['epoch_df']['avg_loss'].min(),
                'total_improvement': exp['gradient_analysis']['total_improvement'],
                'gradient_ratio': exp['gradient_analysis']['gradient_ratio'],
                'convergence_status': exp['gradient_analysis']['convergence_status'],
                'avg_batch_std': exp['epoch_stats']['std'].mean(),
                'avg_batch_var': exp['epoch_stats']['var'].mean(),
            })
        
        comparison_df = pd.DataFrame(comparison_data)
        
        # í†µí•© CSV ë””ë ‰í† ë¦¬ ìƒì„±
        consolidated_csv_dir = self.output_base_dir / "consolidated_csv"
        consolidated_csv_dir.mkdir(exist_ok=True)
        
        # ë¹„êµ CSV ì €ì¥
        comparison_csv = consolidated_csv_dir / "experiments_comparison.csv"
        comparison_df.to_csv(comparison_csv, index=False, encoding='utf-8')
        
        # ëª¨ë“  ì‹¤í—˜ì˜ CSV ë°ì´í„°ë¥¼ í†µí•©
        self.consolidate_all_csv_data(processed_experiments, consolidated_csv_dir)
        
        # ë¹„êµ ë¦¬í¬íŠ¸ ìƒì„±
        report_path = self.output_base_dir / "comparison_report.md"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(f"# ì „ì²´ ì‹¤í—˜ ë¹„êµ ë¦¬í¬íŠ¸\n\n")
            f.write(f"**ìƒì„± ì¼ì‹œ**: {datetime.now().strftime('%Yë…„ %mì›” %dì¼ %Hì‹œ %Më¶„')}\n")
            f.write(f"**ë¶„ì„ ì‹¤í—˜ ìˆ˜**: {len(processed_experiments)}\n\n")
            
            # ê¸°ìš¸ê¸° ì†Œì‹¤ ê¸°ì¤€ ì •ë ¬
            sorted_by_gradient = comparison_df.sort_values('total_improvement')
            
            f.write("## ğŸ¯ ì‹¤í—˜ ì„±ëŠ¥ ìˆœìœ„ (ê¸°ìš¸ê¸° ì†Œì‹¤ ê¸°ì¤€)\n\n")
            f.write("| ìˆœìœ„ | ì‹¤í—˜ ID | ê¸°ìš¸ê¸° ë¹„ìœ¨ | ìˆ˜ë ´ ìƒíƒœ | ì´ ê°œì„ ëŸ‰ |\n")
            f.write("|------|---------|-------------|-----------|----------|\n")
            
            for i, (_, row) in enumerate(sorted_by_gradient.iterrows(), 1):
                f.write(f"| {i} | {row['experiment_id']} | {row['gradient_ratio']:.4f} | {row['convergence_status']} | {row['total_improvement']:.6f} |\n")
            
            # ìµœê³ /ìµœì•… ì‹¤í—˜
            best_exp = sorted_by_gradient.iloc[-1]  # ìµœì¢… ê°œì„ ëŸ‰ì´ ì¢‹ì€ ê²ƒ
            worst_exp = sorted_by_gradient.iloc[0]   # ê·¸ ë°˜ëŒ€
            
            f.write(f"\n## ğŸ† ìµœê³  ì„±ëŠ¥ ì‹¤í—˜\n\n")
            f.write(f"**ì‹¤í—˜ ID**: {best_exp['experiment_id']}\n")
            f.write(f"- ê¸°ìš¸ê¸° ë¹„ìœ¨: {best_exp['gradient_ratio']:.4f}\n")
            f.write(f"- ìˆ˜ë ´ ìƒíƒœ: {best_exp['convergence_status']}\n")
            f.write(f"- ì´ ê°œì„ ëŸ‰: {best_exp['total_improvement']:.6f}\n")
            
            f.write(f"\n## âš ï¸ ê°œì„  í•„ìš” ì‹¤í—˜\n\n")
            f.write(f"**ì‹¤í—˜ ID**: {worst_exp['experiment_id']}\n")
            f.write(f"- ê¸°ìš¸ê¸° ë¹„ìœ¨: {worst_exp['gradient_ratio']:.4f}\n")
            f.write(f"- ìˆ˜ë ´ ìƒíƒœ: {worst_exp['convergence_status']}\n")
            f.write(f"- ì´ ê°œì„ ëŸ‰: {worst_exp['total_improvement']:.6f}\n")
            
            f.write(f"\n## ğŸ“Š ì „ì²´ í†µê³„\n\n")
            f.write(f"- **í‰ê·  ê¸°ìš¸ê¸° ë¹„ìœ¨**: {comparison_df['gradient_ratio'].mean():.4f}\n")
            f.write(f"- **í‰ê·  ì´ ê°œì„ ëŸ‰**: {comparison_df['total_improvement'].mean():.6f}\n")
            f.write(f"- **í‰ê·  ìµœì¢… ì†ì‹¤**: {comparison_df['final_loss'].mean():.6f}\n")
            
            f.write(f"\n## ğŸ“ í†µí•© CSV íŒŒì¼\n\n")
            f.write(f"ëª¨ë“  ì‹¤í—˜ì˜ CSV ë°ì´í„°ê°€ `consolidated_csv/` í´ë”ì— í†µí•©ë˜ì—ˆìŠµë‹ˆë‹¤:\n")
            f.write(f"- **ì‹¤í—˜ ë¹„êµ**: `experiments_comparison.csv`\n")
            f.write(f"- **ëª¨ë“  ì—í­ ë°ì´í„°**: `all_epochs_comprehensive.csv`\n")
            f.write(f"- **ëª¨ë“  ì—í­ í†µê³„**: `all_epochs_statistics.csv`\n")
            f.write(f"- **ëª¨ë“  ì‹¤í—˜ ìš”ì•½**: `all_experiments_summary.csv`\n")
        
        print(f"âœ… ë¹„êµ ë¦¬í¬íŠ¸ ìƒì„±: {report_path}")
        print(f"âœ… ë¹„êµ CSV ìƒì„±: {comparison_csv}")
        print(f"âœ… í†µí•© CSV ìƒì„±: {consolidated_csv_dir}")
    
    def consolidate_all_csv_data(self, processed_experiments, output_dir):
        """ëª¨ë“  ì‹¤í—˜ì˜ CSV ë°ì´í„°ë¥¼ í†µí•©"""
        all_epochs_data = []
        all_stats_data = []
        all_summary_data = []
        
        for exp in processed_experiments:
            experiment_id = exp['experiment_id']
            
            # ì—í­ ë°ì´í„° í†µí•©
            epoch_enhanced = exp['epoch_df'].copy()
            epoch_enhanced['gradient'] = exp['gradient_analysis']['dloss_depoch']
            epoch_enhanced['gradient_2nd'] = exp['gradient_analysis']['d2loss_depoch2']
            epoch_enhanced['abs_gradient'] = exp['gradient_analysis']['abs_gradient']
            epoch_enhanced['experiment_id'] = experiment_id
            all_epochs_data.append(epoch_enhanced)
            
            # í†µê³„ ë°ì´í„° í†µí•©
            stats_data = exp['epoch_stats'].copy()
            stats_data['experiment_id'] = experiment_id
            all_stats_data.append(stats_data)
            
            # ìš”ì•½ ë°ì´í„°ëŠ” ì´ë¯¸ ìˆ˜ì§‘ë˜ì–´ ìˆìŒ
        
        # í†µí•© DataFrame ìƒì„± ë° ì €ì¥
        if all_epochs_data:
            all_epochs_df = pd.concat(all_epochs_data, ignore_index=True)
            all_epochs_csv = output_dir / "all_epochs_comprehensive.csv"
            all_epochs_df.to_csv(all_epochs_csv, index=False, encoding='utf-8')
            print(f"âœ… í†µí•© ì—í­ ë°ì´í„°: {all_epochs_csv}")
        
        if all_stats_data:
            all_stats_df = pd.concat(all_stats_data, ignore_index=True)
            all_stats_csv = output_dir / "all_epochs_statistics.csv"
            all_stats_df.to_csv(all_stats_csv, index=False, encoding='utf-8')
            print(f"âœ… í†µí•© í†µê³„ ë°ì´í„°: {all_stats_csv}")
        
        # ìš”ì•½ ë°ì´í„° í†µí•© (ê°œë³„ ìš”ì•½ íŒŒì¼ë“¤ì„ ì½ì–´ì„œ í†µí•©)
        all_summary_data = []
        for exp in processed_experiments:
            summary_file = exp['csv_files']['summary']
            if summary_file.exists():
                summary_df = pd.read_csv(summary_file)
                all_summary_data.append(summary_df)
        
        if all_summary_data:
            all_summary_df = pd.concat(all_summary_data, ignore_index=True)
            all_summary_csv = output_dir / "all_experiments_summary.csv"
            all_summary_df.to_csv(all_summary_csv, index=False, encoding='utf-8')
            print(f"âœ… í†µí•© ìš”ì•½ ë°ì´í„°: {all_summary_csv}")
    
    def run(self, experiment_ids=None, latest_only=False):
        """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
        print("ğŸš€ í†µí•© ëª¨ë¸ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì‹œì‘")
        print(f"ğŸ“‚ ì‘ì—… ë””ë ‰í† ë¦¬: {self.workspace_dir}")
        print(f"ğŸ’¾ ì¶œë ¥ ë””ë ‰í† ë¦¬: {self.output_base_dir}")
        
        # ì‹¤í—˜ ë°œê²¬
        experiments = self.discover_experiments()
        if not experiments:
            print("âŒ ë¶„ì„í•  ì‹¤í—˜ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ì‹¤í—˜ í•„í„°ë§
        if latest_only:
            experiments = experiments[:1]
            print(f"ğŸ¯ ìµœì‹  ì‹¤í—˜ë§Œ ì²˜ë¦¬: {experiments[0]['id']}")
        elif experiment_ids:
            experiments = [exp for exp in experiments if exp['id'] in experiment_ids]
            print(f"ğŸ¯ ì§€ì •ëœ ì‹¤í—˜ë§Œ ì²˜ë¦¬: {[exp['id'] for exp in experiments]}")
        
        if not experiments:
            print("âŒ ì²˜ë¦¬í•  ì‹¤í—˜ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ê° ì‹¤í—˜ ì²˜ë¦¬
        processed_experiments = []
        for experiment in experiments:
            try:
                result = self.process_experiment(experiment)
                if result:
                    processed_experiments.append(result)
            except Exception as e:
                print(f"âŒ {experiment['id']} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        
        # ì „ì²´ ë¹„êµ ë¦¬í¬íŠ¸ ìƒì„±
        if len(processed_experiments) > 1:
            self.generate_comparison_report(processed_experiments)
        
        print(f"\nğŸ‰ ëª¨ë“  ì²˜ë¦¬ ì™„ë£Œ!")
        print(f"ğŸ“ ê²°ê³¼ ìœ„ì¹˜: {self.output_base_dir}")
        print(f"âœ… ì²˜ë¦¬ëœ ì‹¤í—˜: {len(processed_experiments)}ê°œ")

def main():
    parser = argparse.ArgumentParser(description='í†µí•© ëª¨ë¸ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ')
    parser.add_argument('--workspace', default=os.getcwd(),
                       help='ì‘ì—… ë””ë ‰í† ë¦¬ ê²½ë¡œ (ê¸°ë³¸ê°’: í˜„ì¬ ë””ë ‰í† ë¦¬)')
    parser.add_argument('--output-dir', default='model_result_monitoring',
                       help='ì¶œë ¥ ë””ë ‰í† ë¦¬ ì´ë¦„ (ê¸°ë³¸ê°’: model_result_monitoring)')
    parser.add_argument('--experiments', nargs='+',
                       help='ì²˜ë¦¬í•  íŠ¹ì • ì‹¤í—˜ IDë“¤ (ê¸°ë³¸ê°’: ëª¨ë“  ì‹¤í—˜)')
    parser.add_argument('--latest-only', action='store_true',
                       help='ìµœì‹  ì‹¤í—˜ë§Œ ì²˜ë¦¬')
    
    args = parser.parse_args()
    
    # ëª¨ë‹ˆí„° ìƒì„± ë° ì‹¤í–‰
    monitor = ModelMonitor(workspace_dir=args.workspace, output_base_dir=args.output_dir)
    monitor.run(experiment_ids=args.experiments, latest_only=args.latest_only)

if __name__ == "__main__":
    main()
